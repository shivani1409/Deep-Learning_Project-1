{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Neural Networks with PyTorch\n",
        "\n",
        "In this assignment, we are going to train a Neural Networks on the Japanese MNIST dataset. It is composed of 70000 images of handwritten Hiragana characters. The target variables has 10 different classes.\n",
        "\n",
        "Each image is of dimension 28 by 28. But we will flatten them to form a dataset composed of vectors of dimension (784, 1). The training process will be similar as for a structured dataset.\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=16TqEl9ESfXYbUpVafXD6h5UpJYGKfMxE' width=\"500\" height=\"200\">\n",
        "\n",
        "Your goal is to run at least 3 experiments and get a model that can achieve 80% accuracy with not much overfitting on this dataset.\n",
        "\n",
        "Some of the code have already been defined for you. You need only to add your code in the sections specified (marked with **TODO**). Some assert statements have been added to verify the expected outputs are correct. If it does throw an error, this means your implementation is behaving as expected.\n",
        "\n",
        "Note: You can only use fully-connected and dropout layers for this assignment. You can not convolution layers for instance"
      ],
      "metadata": {
        "id": "KNyZ-zZxlU6G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Import Required Packages"
      ],
      "metadata": {
        "id": "iOufKqO8mw7n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[1.1] We are going to use numpy, matplotlib and google.colab packages"
      ],
      "metadata": {
        "id": "b-sGJ26pmz4A"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oTGG80etnMAa"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Download Dataset\n",
        "\n",
        "We will store the dataset into your personal Google Drive.\n"
      ],
      "metadata": {
        "id": "Vyky0K3fnEFO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[2.1] Mount Google Drive"
      ],
      "metadata": {
        "id": "ltUMtjG-nX-b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "id": "N_FVrXICnMJM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[2.2] Create a folder called `DL_ASG_1` on your Google Drive at the root level"
      ],
      "metadata": {
        "id": "CzLtlKCHnT9H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! mkdir -p /content/gdrive/MyDrive/DL_ASG_1"
      ],
      "metadata": {
        "id": "XZicoPks4POW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[2.3] Navigate to this folder"
      ],
      "metadata": {
        "id": "sToej_3CnePP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd '/content/gdrive/MyDrive/DL_ASG_1'"
      ],
      "metadata": {
        "id": "g2oAXToKnpXj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[2.4] Show the list of item on the folder"
      ],
      "metadata": {
        "id": "TnRHIyhlzUwL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "id": "Y-xYtezBzQ0c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[2.4] Dowload the dataset files to your Google Drive if required"
      ],
      "metadata": {
        "id": "3vlfobqnnjJ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from tqdm import tqdm\n",
        "import os.path\n",
        "\n",
        "def download_file(url):\n",
        "    path = url.split('/')[-1]\n",
        "    if os.path.isfile(path):\n",
        "        print (f\"{path} already exists\")\n",
        "    else:\n",
        "      r = requests.get(url, stream=True)\n",
        "      with open(path, 'wb') as f:\n",
        "          total_length = int(r.headers.get('content-length'))\n",
        "          print('Downloading {} - {:.1f} MB'.format(path, (total_length / 1024000)))\n",
        "          for chunk in tqdm(r.iter_content(chunk_size=1024), total=int(total_length / 1024) + 1, unit=\"KB\"):\n",
        "              if chunk:\n",
        "                  f.write(chunk)\n",
        "\n",
        "url_list = [\n",
        "    'http://codh.rois.ac.jp/kmnist/dataset/kmnist/kmnist-train-imgs.npz',\n",
        "    'http://codh.rois.ac.jp/kmnist/dataset/kmnist/kmnist-train-labels.npz',\n",
        "    'http://codh.rois.ac.jp/kmnist/dataset/kmnist/kmnist-test-imgs.npz',\n",
        "    'http://codh.rois.ac.jp/kmnist/dataset/kmnist/kmnist-test-labels.npz'\n",
        "]\n",
        "\n",
        "for url in url_list:\n",
        "    download_file(url)"
      ],
      "metadata": {
        "id": "M0owzTC427NM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[2.5] List the content of the folder and confirm files have been dowloaded properly"
      ],
      "metadata": {
        "id": "DVF_Cx7Hny2i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! ls"
      ],
      "metadata": {
        "id": "vt6ZKf4fnqkq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Load Data"
      ],
      "metadata": {
        "id": "fvvfOON36hTf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[3.1] Import the required modules from PyTorch"
      ],
      "metadata": {
        "id": "duFjgsyPoLPR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO (Students need to fill this section)\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import torchvision.transforms as transforms\n"
      ],
      "metadata": {
        "id": "1zolHKEO7GZA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[3.2] **TODO** Create 2 variables called `img_height` and `img_width` that will both take the value 28"
      ],
      "metadata": {
        "id": "r4Aw5ObQoWdI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO (Students need to fill this section)\n",
        "img_height = 28\n",
        "img_width =28"
      ],
      "metadata": {
        "id": "Ip0NFeyjpj79"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[3.3] Create a function that loads a .npz file using numpy and return the content of the `arr_0` key"
      ],
      "metadata": {
        "id": "hmX5SEHkpp63"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load(f):\n",
        "    return np.load(f)['arr_0']"
      ],
      "metadata": {
        "id": "5S3cthx57L2f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[3.4] **TODO** Load the 4 files saved on your Google Drive into their respective variables: x_train, y_train, x_test and y_test"
      ],
      "metadata": {
        "id": "8V2Ij9s7qRtj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO (Students need to fill this section)\n",
        "x_train = load('/content/gdrive/MyDrive/DL_ASG_1/kmnist-train-imgs.npz')\n",
        "y_train = load('/content/gdrive/MyDrive/DL_ASG_1/kmnist-train-labels.npz')\n",
        "x_test = load('/content/gdrive/MyDrive/DL_ASG_1/kmnist-test-imgs.npz')\n",
        "y_test = load('/content/gdrive/MyDrive/DL_ASG_1/kmnist-test-labels.npz')\n"
      ],
      "metadata": {
        "id": "5XTkRb0lqpEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[3.5] **TODO** Using matplotlib display the first image from the train set and its target value"
      ],
      "metadata": {
        "id": "3KC12nB7rlbV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO (Students need to fill this section)\n",
        "plt.imshow(x_train[0].reshape(img_height, img_width), cmap='gray')\n",
        "plt.title(f'Label: {y_train[0]}')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "AOtWg7bBrwmV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Prepare Data"
      ],
      "metadata": {
        "id": "htLk_27ir0B1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[4.1] **TODO** Reshape the images from the training and testing set to have the channel dimension last. The dimensions should be: (row_number, height, width, channel)"
      ],
      "metadata": {
        "id": "VJEBe30Er33P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO (Students need to fill this section)\n",
        "x_train = x_train.reshape(-1, 1, img_height, img_width)\n",
        "x_test = x_test.reshape(-1, 1, img_height, img_width)\n"
      ],
      "metadata": {
        "id": "1yqWleZasxdR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[4.2] **TODO** Cast `x_train` and `x_test` into `float32` decimals"
      ],
      "metadata": {
        "id": "F2f6wvFys2ZI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = torch.tensor(x_train, dtype=torch.float32) / 255.0\n",
        "x_test = torch.tensor(x_test, dtype=torch.float32) / 255.0\n"
      ],
      "metadata": {
        "id": "FWZmWe73tLXT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[4.3] **TODO** Standardise the images of the training and testing sets. Originally each image contains pixels with value ranging from 0 to 255. after standardisation, the new value range should be from 0 to 1."
      ],
      "metadata": {
        "id": "Z-1Jr0pKs6jv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO (Students need to fill this section)\n",
        "# Standardize images in training set\n",
        "x_train = x_train / 255.0\n",
        "\n",
        "# Standardize images in testing set\n",
        "x_test = x_test / 255.0\n"
      ],
      "metadata": {
        "id": "RXY1o272t0JO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[4.4] **TODO** Create a variable called `num_classes` that will take the value 10 which corresponds to the number of classes for the target variable"
      ],
      "metadata": {
        "id": "9eH4aZmXt7Fe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO (Students need to fill this section)\n",
        "num_classes =10"
      ],
      "metadata": {
        "id": "gTnMgLxYuUs6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[4.5] **TODO** Convert the target variable for the training and testing sets to a binary class matrix of dimension (rows, num_classes).\n",
        "\n",
        "For example:\n",
        "- class 0 will become [1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
        "- class 1 will become [0, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
        "- class 5 will become [0, 0, 0, 0, 0, 1, 0, 0, 0, 0]\n",
        "- class 9 will become [0, 0, 0, 0, 0, 0, 0, 0, 0, 1]"
      ],
      "metadata": {
        "id": "iAy0fUJsuyhb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO (Students need to fill this section)\n",
        "y_train = torch.nn.functional.one_hot(torch.tensor(y_train).to(torch.int64), num_classes)\n",
        "y_test = torch.nn.functional.one_hot(torch.tensor(y_test).to(torch.int64), num_classes)\n"
      ],
      "metadata": {
        "id": "ysNg37Ukwq8S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Define Neural Networks Architecure"
      ],
      "metadata": {
        "id": "0OCorS00wxPN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[5.1] Set the seed in PyTorch for reproducing results\n",
        "\n"
      ],
      "metadata": {
        "id": "7G_L-yqTxI1d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO (Students need to fill this section)\n",
        "\n",
        "torch.manual_seed(42)  # Example seed\n"
      ],
      "metadata": {
        "id": "XB8OIC9wrgFG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[5.2] **TODO** Define the architecture of your Neural Networks and save it into a variable called `model`"
      ],
      "metadata": {
        "id": "5b93U4MixWeE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Define three neural network architectures in a single code block.\n",
        "\n",
        "# Neural Network 1: Basic Fully Connected Network\n",
        "class NeuralNet(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(NeuralNet, self).__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.linear_relu_stack = nn.Sequential(\n",
        "            nn.Linear(28*28, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(512, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        logits = self.linear_relu_stack(x)\n",
        "        return logits\n",
        "\n",
        "# Neural Network 2 : Network with Batch Normalization and Dropout\n",
        "class CustomNet(nn.Module):\n",
        "    def __init__(self, input_size=784, num_classes=10):\n",
        "        super(CustomNet, self).__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.linear_relu_stack = nn.Sequential(\n",
        "            nn.Linear(input_size, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.25),  # Adjust dropout rate to prevent overfitting\n",
        "            nn.Linear(512, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.25),  # Adjust dropout rate to prevent overfitting\n",
        "            nn.Linear(256, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.25),  # Adjust dropout rate to prevent overfitting\n",
        "            nn.Linear(128, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        logits = self.linear_relu_stack(x)\n",
        "        return logits\n",
        "\n",
        "# Neural Network 3 :Advanced Network with Layer Normalization and LeakyReLU\n",
        "class DeepCustomNet(nn.Module):\n",
        "    def __init__(self, input_size=784, num_classes=10):\n",
        "        super(DeepCustomNet, self).__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.network = nn.Sequential(\n",
        "            nn.Linear(input_size, 1024),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),  # Prevent overfitting\n",
        "            nn.Linear(1024, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),  # Further prevent overfitting\n",
        "            nn.Linear(512, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),  # Increased dropout for deeper layer\n",
        "            nn.Linear(256, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),  # Consistent with prior layer to manage complexity\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.4),  # Higher dropout in a deeper section\n",
        "            nn.Linear(64, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        logits = self.network(x)\n",
        "        return logits\n",
        "\n",
        "# Instantiate and print the summaries of each model\n",
        "model1 = NeuralNet()\n",
        "model2 = CustomNet()\n",
        "model3 = DeepCustomNet()\n",
        "\n",
        "model_summaries = [model1, model2, model3]\n",
        "\n",
        "# Instead of printing model summaries (which would be too verbose and not insightful in this format),\n",
        "# let's just confirm the models are created by printing their class types.\n",
        "[model.__class__.__name__ for model in model_summaries]\n"
      ],
      "metadata": {
        "id": "gq1f74uKxpkp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[5.2] **TODO** Print the summary of your model"
      ],
      "metadata": {
        "id": "0IvuMQ81xu5U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO (Students need to fill this section)\n",
        "print(model1)\n",
        "print(model2)\n",
        "print(model3)"
      ],
      "metadata": {
        "id": "gBRm-h5dxvIw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Train Neural Networks"
      ],
      "metadata": {
        "id": "sOPTnNxtx6MC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[6.1] **TODO** Create 2 variables called `batch_size` and `epochs` that will  respectively take the values 128 and 500"
      ],
      "metadata": {
        "id": "fsHJzhnAyP4H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO (Students need to fill this section)\n",
        "batch_size =128\n",
        "epochs =500"
      ],
      "metadata": {
        "id": "hNe_Cia0yde-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[6.2] **TODO** Compile your model with the appropriate loss function, the optimiser of your choice and the accuracy metric"
      ],
      "metadata": {
        "id": "4-bAkzwXyjAs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO (Students need to fill this section)\n",
        "import torch.optim as optim\n",
        "\n",
        "# Define optimizers and loss functions for each of the three models.\n",
        "\n",
        "# For model 1 (NeuralNet)\n",
        "optimizer1 = optim.Adam(model1.parameters(), lr=0.001)\n",
        "criterion1 = nn.CrossEntropyLoss()\n",
        "\n",
        "# For model 2 (CustomNet)\n",
        "optimizer2 = optim.Adam(model2.parameters(), lr=0.001)\n",
        "criterion2 = nn.CrossEntropyLoss()\n",
        "\n",
        "# For model 3 (DeepCustomNet)\n",
        "optimizer3 = optim.Adam(model3.parameters(), lr=0.001)\n",
        "criterion3 = nn.CrossEntropyLoss()\n",
        "\n",
        "# Let's confirm the optimizer types and loss function types for each model.\n",
        "{\n",
        "    \"optimizer1_type\": type(optimizer1).__name__,\n",
        "    \"criterion1_type\": type(criterion1).__name__,\n",
        "    \"optimizer2_type\": type(optimizer2).__name__,\n",
        "    \"criterion2_type\": type(criterion2).__name__,\n",
        "    \"optimizer3_type\": type(optimizer3).__name__,\n",
        "    \"criterion3_type\": type(criterion3).__name__,\n",
        "}\n"
      ],
      "metadata": {
        "id": "0WnNAYT6yjci"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[6.3] **TODO** Train your model\n",
        "using the number of epochs defined. Calculate the total loss and save it to a variable called total_loss."
      ],
      "metadata": {
        "id": "iRvM_pEZy7SX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "    # TODO (Students need to fill this section)For model 1\n",
        "total_loss = 0.0  # Initialize total loss\n",
        "for epoch in range(epochs):\n",
        "     model1.train()  # Set model to training mode\n",
        "    running_loss = 0.0\n",
        "    for images, labels in DataLoader(TensorDataset(x_train, y_train), batch_size=batch_size, shuffle=True):\n",
        "        optimizer1.zero_grad()  # Zero the gradients\n",
        "        outputs = model1(images)  # Forward pass\n",
        "        loss = criterion1(outputs, torch.max(labels, 1)[1])  # Calculate loss\n",
        "        loss.backward()  # Backward pass\n",
        "        optimizer1.step()  # Optimize\n",
        "        running_loss += loss.item() * images.size(0)  # Multiply loss by batch size\n",
        "    epoch_loss = running_loss / len(x_train)  # Calculate average loss per epoch\n",
        "    total_loss += epoch_loss  # Update total loss\n",
        "    print(f\"Epoch {epoch+1}, Loss: {epoch_loss}\")\n",
        "print(f\"Total loss over all epochs: {total_loss}\")"
      ],
      "metadata": {
        "id": "EMzFo2r5JKn6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "    # TODO (Students need to fill this section)For model 2\n",
        "total_loss = 0.0  # Initialize total loss\n",
        "for epoch in range(epochs):\n",
        "     model2.train()  # Set model to training mode\n",
        "    running_loss = 0.0\n",
        "    for images, labels in DataLoader(TensorDataset(x_train, y_train), batch_size=batch_size, shuffle=True):\n",
        "        optimizer2.zero_grad()  # Zero the gradients\n",
        "        outputs = model2(images)  # Forward pass\n",
        "        loss = criterion2(outputs, torch.max(labels, 1)[1])  # Calculate loss\n",
        "        loss.backward()  # Backward pass\n",
        "        optimizer2.step()  # Optimize\n",
        "        running_loss += loss.item() * images.size(0)  # Multiply loss by batch size\n",
        "    epoch_loss = running_loss / len(x_train)  # Calculate average loss per epoch\n",
        "    total_loss += epoch_loss  # Update total loss\n",
        "    print(f\"Epoch {epoch+1}, Loss: {epoch_loss}\")\n",
        "print(f\"Total loss over all epochs: {total_loss}\")"
      ],
      "metadata": {
        "id": "_qy9mijatfHu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "    # TODO (Students need to fill this section)For model 3\n",
        "total_loss = 0.0  # Initialize total loss\n",
        "for epoch in range(epochs):\n",
        "     model3.train()  # Set model to training mode\n",
        "    running_loss = 0.0\n",
        "    for images, labels in DataLoader(TensorDataset(x_train, y_train), batch_size=batch_size, shuffle=True):\n",
        "        optimizer3.zero_grad()  # Zero the gradients\n",
        "        outputs = model3(images)  # Forward pass\n",
        "        loss = criterion3(outputs, torch.max(labels, 1)[1])  # Calculate loss\n",
        "        loss.backward()  # Backward pass\n",
        "        optimizer3.step()  # Optimize\n",
        "        running_loss += loss.item() * images.size(0)  # Multiply loss by batch size\n",
        "    epoch_loss = running_loss / len(x_train)  # Calculate average loss per epoch\n",
        "    total_loss += epoch_loss  # Update total loss\n",
        "    print(f\"Epoch {epoch+1}, Loss: {epoch_loss}\")\n",
        "print(f\"Total loss over all epochs: {total_loss}\")"
      ],
      "metadata": {
        "id": "CNIi2KefttP0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_and_evaluate_model(model, optimizer, criterion, x_train, y_train, epochs, batch_size):\n",
        "    epoch_losses = []  # Initialize a list to store the average loss of each epoch\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()  # Set model to training mode\n",
        "        running_loss = 0.0\n",
        "\n",
        "        # Iterate over the DataLoader for training data\n",
        "        for images, labels in DataLoader(TensorDataset(x_train, y_train), batch_size=batch_size, shuffle=True):\n",
        "            optimizer.zero_grad()  # Zero the gradients\n",
        "            outputs = model(images)  # Forward pass\n",
        "\n",
        "            # Calculate loss; adjust if your labels are not one-hot encoded\n",
        "            loss = criterion(outputs, labels.argmax(dim=1))\n",
        "            loss.backward()  # Backward pass\n",
        "            optimizer.step()  # Optimize\n",
        "\n",
        "            # Update the running loss\n",
        "            running_loss += loss.item() * images.size(0)\n",
        "\n",
        "        # Calculate the average loss for the epoch\n",
        "        epoch_loss = running_loss / len(x_train)\n",
        "        epoch_losses.append(epoch_loss)  # Append the average loss for this epoch to the list\n",
        "\n",
        "        print(f\"Model {model.__class__.__name__}: Epoch {epoch+1}/{epochs}, Loss: {epoch_loss:.4f}\")\n",
        "\n",
        "    # Calculate and print the total loss across all epochs for the current model\n",
        "    total_loss = sum(epoch_losses)\n",
        "    print(f\"Model {model.__class__.__name__}: Total loss over all epochs: {total_loss:.4f}\\n\")\n",
        "    return epoch_losses  # Optionally return the list of epoch losses\n",
        "\n",
        "# Assuming x_train, y_train, epochs, and batch_size are already defined,\n",
        "# and model1, model2, model3, optimizer1, optimizer2, optimizer3, criterion1, criterion2, criterion3 are properly initialized.\n",
        "epoch_losses_model1 = train_and_evaluate_model(model1, optimizer1, criterion1, x_train, y_train, epochs, batch_size)\n",
        "epoch_losses_model2 = train_and_evaluate_model(model2, optimizer2, criterion2, x_train, y_train, epochs, batch_size)\n",
        "epoch_losses_model3 = train_and_evaluate_model(model3, optimizer3, criterion3, x_train, y_train, epochs, batch_size)\n",
        "\n",
        "# Now, if you wish to plot the training losses for each model, you can use the stored `epoch_losses_model1`, `epoch_losses_model2`, `epoch_losses_model3`.\n"
      ],
      "metadata": {
        "id": "PF0oa0z-jt0u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[6.4] **TODO** Test your model.  Initiate the model.eval() along with torch.no_grad() to turn off the gradients.\n"
      ],
      "metadata": {
        "id": "emZ5Ayr88PZh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, x_train, y_train, x_test, y_test, batch_size):\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    total_train, correct_train, total_test, correct_test = 0, 0, 0, 0\n",
        "\n",
        "    with torch.no_grad():  # Turn off gradients for evaluation\n",
        "        # Evaluate on training data\n",
        "        for images, labels in DataLoader(TensorDataset(x_train, y_train), batch_size=batch_size):\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total_train += labels.size(0)\n",
        "            correct_train += (predicted == labels.argmax(dim=1)).sum().item()\n",
        "\n",
        "        # Evaluate on testing data\n",
        "        for images, labels in DataLoader(TensorDataset(x_test, y_test), batch_size=batch_size):\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total_test += labels.size(0)\n",
        "            correct_test += (predicted == labels.argmax(dim=1)).sum().item()\n",
        "\n",
        "    # Calculate and print accuracy\n",
        "    train_accuracy = correct_train / total_train * 100\n",
        "    test_accuracy = correct_test / total_test * 100\n",
        "    print(f'Model {model.__class__.__name__} Training Accuracy: {train_accuracy:.2f}%')\n",
        "    print(f'Model {model.__class__.__name__} Testing Accuracy: {test_accuracy:.2f}%\\n')\n",
        "\n",
        "# Assuming x_train, y_train, x_test, y_test, and batch_size are defined,\n",
        "# and model1, model2, model3 are properly initialized.\n",
        "evaluate_model(model1, x_train, y_train, x_test, y_test, batch_size)\n",
        "evaluate_model(model2, x_train, y_train, x_test, y_test, batch_size)\n",
        "evaluate_model(model3, x_train, y_train, x_test, y_test, batch_size)\n"
      ],
      "metadata": {
        "id": "bfvBZ3zy8QM9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7. Analyse Results"
      ],
      "metadata": {
        "id": "vz9uFy_X6oeA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[7.1] **TODO** Display the performance of your model on the training and testing sets"
      ],
      "metadata": {
        "id": "ddugPZhZ68Wb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_model(model1, x_train, y_train, x_test, y_test, batch_size)\n",
        "evaluate_model(model2, x_train, y_train, x_test, y_test, batch_size)\n",
        "evaluate_model(model3, x_train, y_train, x_test, y_test, batch_size)\n"
      ],
      "metadata": {
        "id": "yihZIPZ_6sql"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[7.2] **TODO** Plot the learning curve of your model"
      ],
      "metadata": {
        "id": "iBTo_xEI7K_z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "epochs_range = range(1, epochs + 1)\n",
        "\n",
        "plt.figure(figsize=(12, 7))  # Adjust the figure size as necessary\n",
        "\n",
        "# Plot for Model 1\n",
        "plt.plot(epochs_range, epoch_losses_model1, label='Model 1 Training Loss')\n",
        "\n",
        "# Plot for Model 2\n",
        "plt.plot(epochs_range, epoch_losses_model2, label='Model 2 Training Loss')\n",
        "\n",
        "# Plot for Model 3\n",
        "plt.plot(epochs_range, epoch_losses_model3, label='Model 3 Training Loss')\n",
        "\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training Loss Over Epochs for All Models')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "jRt_4W2F7RVV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[7.3] **TODO** Display the confusion matrix on the testing set predictions"
      ],
      "metadata": {
        "id": "qKPu98GR7a17"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "def plot_confusion_matrix(model, x_test, y_test, batch_size, num_classes):\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    y_true = [label.argmax().item() for label in y_test]\n",
        "    y_pred = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, _ in DataLoader(TensorDataset(x_test, y_test), batch_size=batch_size):\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            y_pred.extend(predicted.tolist())\n",
        "\n",
        "    cm = confusion_matrix(y_true, y_pred, labels=range(num_classes))\n",
        "\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    sns.heatmap(cm, annot=True, fmt='g', cmap='Blues', xticklabels=range(1, num_classes + 1), yticklabels=range(1, num_classes + 1))\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('True')\n",
        "    plt.title(f'Confusion Matrix for {model.__class__.__name__}')\n",
        "    plt.show()\n",
        "\n",
        "# Assuming x_test, y_test, batch_size, and num_classes are defined and correct\n",
        "# Call the function for each model\n",
        "plot_confusion_matrix(model1, x_test, y_test, batch_size, num_classes)\n",
        "plot_confusion_matrix(model2, x_test, y_test, batch_size, num_classes)\n",
        "plot_confusion_matrix(model3, x_test, y_test, batch_size, num_classes)\n"
      ],
      "metadata": {
        "id": "TkrP9JCgMzpT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}